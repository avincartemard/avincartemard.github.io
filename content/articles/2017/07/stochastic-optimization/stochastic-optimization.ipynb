{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a world where data can be collected continuously and storage costs are cheap, issues related to the growing size of interesting datasets can pose a problem unless we have the right tools for the task. Indeed, in the event where we have *streaming data* it might be impossible to wait until the \"end\" before fitting our model since it may never come. Alternatively it might be problematic to even store all of the data, scattered across many different servers, in memory before using it. Instead it would be preferable to do an update each time some new data (or a small batch of it) arrives. Similarly we might find ourselves in an offline situation where the number of training examples is *very large* and traditional approaches, such as gradient descent, start to become too slow for our needs. \n",
    "\n",
    "**Stochastic gradient descent** (SGD) offers an easy solution to all of these problems. \n",
    "\n",
    "In this post we explore the convergence properties of stochastic gradient descent and a few of its variants, namely\n",
    "\n",
    "* Polyak-Ruppert averaged stochastic gradient;\n",
    "* Adaptive Gradient, also known as AdaGrad;\n",
    "* Stochastic Average Gradient, also known as SAG.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Stochastic Gradient Descent for Logistic Regression\n",
    "\n",
    "In this post we will consider the classification dataset [`quantum.mat`]({filename}/datasets/quantum.mat) that I used during an assignment for the course CPSC 540 - Machine Learning at UBC. It contains a matrix $X$ with dimensions $50000 \\times 79$, as well as a vector $y$ taking values $\\pm 1$, which we will classify using logistic regression. The true minimum of our model's cost function lies at $f(w) = 2.7068 \\times 10^4$, but various implementations of SGD have drastically different performances as we will see. \n",
    "\n",
    "The cost function for logistic regression with L2-regularization can be written in a form well-suited to our minimization objective:\n",
    "\n",
    "\\begin{equation}\n",
    "f(w) = \\sum_{i=1}^n f_i(w), \\;\\;\\;\\;\\; \\text{with} \\;\\; f_i(w) = \\log \\Big( 1 + \\exp \\left( - y_i X_{ia} w_a \\right) \\Big) + \\frac{\\lambda}{2 n} w^2.\n",
    "\\end{equation}\n",
    "\n",
    "The key idea behind SGD is to approximate the true gradient $\\nabla f(w)$ by the successive gradients at individual training examples $\\nabla f_i(w)$, which naturally befits online learning. The rationale for doing so is that it is very costly to compute the exact direction of the gradient, whereas a good but noisy estimate can be obtained by looking at a few examples only. This noise has the added benefit of preventing SGD from getting stuck in the shallow local minima that might be present for non-convex optimization objectives (such as neural networks), at the cost of never truly converging to a minimum but rather in a neighborhood around it. \n",
    "\n",
    "We now proceed to build a `LogisticRegressionSGD()` class that implements the logistic regression cost function written above. We will then include different stochastic optimization methods and see how close they can get to the true minimum after 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# load data from MATLAB file\n",
    "datamat = loadmat('quantum.mat')\n",
    "X = datamat['X']\n",
    "y = datamat['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD(object):\n",
    "    \n",
    "    def __init__(self, X, y, progTol=1e-4, nEpochs=10):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        # define convergence parameters here so all methods can use them\n",
    "        self.progTol = progTol\n",
    "        self.nEpochs = nEpochs\n",
    "        \n",
    "    def LogReg(self, w, Lambda, i=None):\n",
    "        \"\"\" Logistic regression cost function with L2-regularization \"\"\"\n",
    "        \"\"\" Outputs negative log-likelihood (nll) and gradient (grad) \"\"\"\n",
    "        if isinstance(i, np.int64):   # for individual training examples\n",
    "            X = self.X[[i], :]\n",
    "            y = self.y[[i]]\n",
    "        else:\n",
    "            X = self.X\n",
    "            y = self.y\n",
    "            \n",
    "        yXw = y*X.dot(w)\n",
    "        sigmoid = 1/(1 + np.exp(-yXw))\n",
    "        \n",
    "        nll = -np.sum(np.log(sigmoid)) + 0.5*Lambda*w.T.dot(w)\n",
    "        grad = -X.T.dot(y*(1-sigmoid)) + Lambda*w\n",
    "        \n",
    "        return nll, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Stochastic Gradient Descent\n",
    "\n",
    "Schematically the standard SGD algorithm takes the form\n",
    "\n",
    "1. Initialize weights $w$ and learning rate $\\eta_t$.\n",
    "\n",
    "2.  - Randomly permute training examples.\n",
    "    \n",
    "    - For $i = 1 : n$ do $w \\leftarrow w - \\eta_t \\nabla f_i(w)$.\n",
    "    \n",
    "3. Repeat step 2 until convergence.\n",
    "\n",
    "Many theory papers use the step size $\\eta_t = 1/\\lambda t$, which offers the best convergence rate in the worst case scenario. However choosing this learning rate typically leads the SGD algorithm in regions of parameter space afflicted by numerical overflow of the cost function before it ultimately \"converges\" to $f(w) \\geq 4.5 \\times 10^4$, far away from the global minimum. \n",
    "\n",
    "Alternatives include choosing a constant learning rate (we found that $\\eta = 10^{-4}$ gave reasonable results) or an iteration-dependent rate that slowly converges to 0, such as \n",
    "\n",
    "\\begin{equation}\n",
    "\\eta_t = \\frac{\\sqrt{n}}{\\sqrt{n} + t}.\n",
    "\\end{equation}\n",
    "\n",
    "We find that the last two approaches yield similar results, although the latter requires the fine-tuning of both the numerator and denominator in order to work optimally, and also makes it hard to decide when to stop since later iterations move very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RegularSGD(self, case):\n",
    "    # Initialize\n",
    "    n = self.n\n",
    "    w = np.zeros((self.d, 1))\n",
    "    w_old = w\n",
    "    Lambda = 1\n",
    "    \n",
    "    # Randomly shuffle training data\n",
    "    arr = np.arange(0, n)\n",
    "    np.random.shuffle(arr) # shuffles arr directly\n",
    "        \n",
    "    for t in range(1, self.nEpochs*n + 1):\n",
    "        # Compute nll and grad for one random training example\n",
    "        nll, grad = self.LogReg(w=w, Lambda=Lambda/n, i=arr[np.mod(t, n)])\n",
    "        if case==1:\n",
    "            eta = 1/(Lambda*t)\n",
    "        elif case==2:\n",
    "            eta = 1e-4\n",
    "        elif case==3:\n",
    "            eta = np.sqrt(n)/(np.sqrt(n) + t)   # step size\n",
    "        else:\n",
    "            print(\"The variable 'case' is not specified correctly; abort.\")\n",
    "            break\n",
    "        w = w - eta*grad   # gradient step\n",
    "            \n",
    "        # One epoch has passed: check for convergence\n",
    "        if np.mod(t, n) == 0:\n",
    "            change = np.linalg.norm(w-w_old, ord=np.inf)\n",
    "            print('Passes = %d, function = %e, change = %f' %((t+1)/n, self.LogReg(w=w, Lambda=Lambda)[0], change))\n",
    "            if change < self.progTol:\n",
    "                print('Parameters changed b less than progress tolerance on pass')\n",
    "                break\n",
    "            np.random.shuffle(arr)   # reshuffle\n",
    "            w_old = w\n",
    "            \n",
    "# Add method to our class\n",
    "LogisticRegressionSGD.RegularSGD = RegularSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "  Case 1: best rate for worst case scenario  \n",
      "---------------------------------------------\n",
      "Passes = 1, function = 6.497187e+04, change = 4.257697\n",
      "Passes = 2, function = 6.332083e+04, change = 0.071927\n",
      "Passes = 3, function = 6.241682e+04, change = 0.041528\n",
      "Passes = 4, function = 6.179935e+04, change = 0.029383\n",
      "Passes = 5, function = 6.133539e+04, change = 0.022594\n",
      "Passes = 6, function = 6.096411e+04, change = 0.018459\n",
      "Passes = 7, function = 6.065550e+04, change = 0.015570\n",
      "Passes = 8, function = 6.039239e+04, change = 0.013437\n",
      "Passes = 9, function = 6.016341e+04, change = 0.011837\n",
      "Passes = 10, function = 5.996084e+04, change = 0.010564\n",
      "---------------------------------------------\n",
      "  Case 2: small and constant step size       \n",
      "---------------------------------------------\n",
      "Passes = 1, function = 2.827318e+04, change = 0.428875\n",
      "Passes = 2, function = 2.767742e+04, change = 0.148022\n",
      "Passes = 3, function = 2.744054e+04, change = 0.082473\n",
      "Passes = 4, function = 2.731625e+04, change = 0.055343\n",
      "Passes = 5, function = 2.723884e+04, change = 0.045643\n",
      "Passes = 6, function = 2.719352e+04, change = 0.038638\n",
      "Passes = 7, function = 2.715925e+04, change = 0.037987\n",
      "Passes = 8, function = 2.713423e+04, change = 0.027871\n",
      "Passes = 9, function = 2.711764e+04, change = 0.023492\n",
      "Passes = 10, function = 2.710850e+04, change = 0.024259\n",
      "---------------------------------------------\n",
      "  Case 3: monotonously decreasing step size  \n",
      "---------------------------------------------\n",
      "Passes = 1, function = 2.775579e+04, change = 0.995766\n",
      "Passes = 2, function = 2.726339e+04, change = 0.156621\n",
      "Passes = 3, function = 2.729197e+04, change = 0.070505\n",
      "Passes = 4, function = 2.724176e+04, change = 0.077701\n",
      "Passes = 5, function = 2.723945e+04, change = 0.052928\n",
      "Passes = 6, function = 2.718166e+04, change = 0.051606\n",
      "Passes = 7, function = 2.711573e+04, change = 0.032869\n",
      "Passes = 8, function = 2.713621e+04, change = 0.056568\n",
      "Passes = 9, function = 2.715035e+04, change = 0.042247\n",
      "Passes = 10, function = 2.709499e+04, change = 0.034909\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------')\n",
    "print('  Case 1: best rate for worst case scenario  ')\n",
    "print('---------------------------------------------')\n",
    "LogisticRegressionSGD(X, y).RegularSGD(case=1)\n",
    "print('---------------------------------------------')\n",
    "print('  Case 2: small and constant step size       ')\n",
    "print('---------------------------------------------')\n",
    "LogisticRegressionSGD(X, y).RegularSGD(case=2)\n",
    "print('---------------------------------------------')\n",
    "print('  Case 3: monotonously decreasing step size  ')\n",
    "print('---------------------------------------------')\n",
    "LogisticRegressionSGD(X, y).RegularSGD(case=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyak-Ruppert Averaged Stochastic Gradient\n",
    "\n",
    "Rather than use the information contained in the weights $w$ at iteration $t$ to determine the descent direction, it is often an improvement to use a *running average* instead, which keeps a memory of previous iterations\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{w}_t = \\overline{w}_{t-1} - \\frac{1}{t} \\left( \\overline{w}_{t-1} - w_t \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Doing so results in a slight improvement over regular SGD. Note that convergence improves if we start averaging after `nAvg` $\\geq 2$ passes in order to smooth out the initial irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AverageSGD(self, nAvg=2):\n",
    "    # Initialize\n",
    "    n = self.n\n",
    "    w = np.zeros((self.d, 1))\n",
    "    w_old = w\n",
    "    w_avg = w   # averaged weights\n",
    "    Lambda = 1\n",
    "    nPasses = 0\n",
    "    \n",
    "    # Randomly shuffle training data\n",
    "    arr = np.arange(0, n)\n",
    "    np.random.shuffle(arr) # shuffles arr directly\n",
    "        \n",
    "    for t in range(1, self.nEpochs*n + 1):\n",
    "        # Compute nll and grad for one random training example\n",
    "        nll, grad = self.LogReg(w=w, Lambda=Lambda/n, i=arr[np.mod(t, n)])\n",
    "        eta = 1e-4   # step size\n",
    "        w = w - eta*grad   # gradient step\n",
    "            \n",
    "        if nPasses >= nAvg:\n",
    "            w_avg = w_avg - 1/(t-nAvg*n+1)*(w_avg - w)\n",
    "            \n",
    "        # One epoch has passed: check for convergence\n",
    "        if np.mod(t, n) == 0:\n",
    "            nPasses = nPasses + 1\n",
    "            change = np.linalg.norm(w-w_old, ord=np.inf)\n",
    "            print('Passes = %d, function = %e, change = %f' %((t+1)/n, self.LogReg(w=w, Lambda=Lambda)[0], change))\n",
    "            if change < self.progTol:\n",
    "                print('Parameters changed b less than progress tolerance on pass')\n",
    "                break\n",
    "            np.random.shuffle(arr)   # reshuffle\n",
    "            w_old = w\n",
    "            \n",
    "LogisticRegressionSGD.AverageSGD = AverageSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes = 1, function = 2.826491e+04, change = 0.430516\n",
      "Passes = 2, function = 2.767581e+04, change = 0.146381\n",
      "Passes = 3, function = 2.744244e+04, change = 0.078595\n",
      "Passes = 4, function = 2.731937e+04, change = 0.058312\n",
      "Passes = 5, function = 2.724000e+04, change = 0.045239\n",
      "Passes = 6, function = 2.719524e+04, change = 0.039755\n",
      "Passes = 7, function = 2.715876e+04, change = 0.033641\n",
      "Passes = 8, function = 2.713543e+04, change = 0.025909\n",
      "Passes = 9, function = 2.711776e+04, change = 0.021958\n",
      "Passes = 10, function = 2.710598e+04, change = 0.020059\n"
     ]
    }
   ],
   "source": [
    "LogisticRegressionSGD(X, y).AverageSGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Gradient (AdaGrad)\n",
    "\n",
    "One of the main drawbacks of the stochastic optimization methods outlined above is the need to manually choose the optimal learning rate for the problem at hand. [AdaGrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf), an algorithm proposed in 2011, eschews this problem by computing an appropriate learning rate for each direction $\\hat{w_a} \\in \\mathbb{R}^d$. \n",
    "\n",
    "AdaGrad automatically assigns a higher learning rate to rare/sparse features, which typically have a higher predictive power than common ones. We can understand this intuitively by thinking about *words* in a story: rare words like *Daenerys* and *dragons* provide significantly more information and context for the audience of Game of Thrones than common ones such as *the* or *a*. Therefore AdaGrad ensures that the most predictive features have larger updates (i.e. the associated weights increase/decrease proportionally to their importance) than the ones providing irrelevant information.\n",
    "\n",
    "The weight update for AdaGrad is given by\n",
    "\n",
    "\\begin{equation}\n",
    "w_{t+1} = w_t - \\eta_t D_t \\nabla f(w_t),\n",
    "\\end{equation}\n",
    "\n",
    "where the diagonal matrix $D_t$ has elements\n",
    "\n",
    "\\begin{equation}\n",
    "(D_t)_{jj} = \\frac{1}{\\sqrt{\\delta + \\sum_{k=0}^t \\nabla_j f_{i_k}(w_k)^2}}.\n",
    "\\end{equation}\n",
    "\n",
    "Here $i_k$ denotes example $i$ chosen randomly on iteration $k$, $\\nabla_j$ is the $j$th element of the gradient, and $\\delta$ is a small number to prevent division by 0. All we need to do is fiddle with the constant learning rate $\\eta_t = \\eta$ since $D_t$ automatically takes care of assigning higher importance to sparse features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaGrad(self, eta = 0.025, delta=1e-3):\n",
    "    # Initialize\n",
    "    n = self.n\n",
    "    w = np.zeros((self.d, 1))\n",
    "    w_old = w\n",
    "    Lambda = 1\n",
    "    \n",
    "    # keep sum of squared gradients in memory\n",
    "    sumGrad_sq = 0\n",
    "    \n",
    "    # Randomly shuffle training data\n",
    "    arr = np.arange(0, n)\n",
    "    np.random.shuffle(arr) # shuffles arr directly\n",
    "    \n",
    "    for t in range(1, self.nEpochs*n + 1):\n",
    "        # Compute nll and grad for one random training example\n",
    "        nll, grad = self.LogReg(w=w, Lambda=Lambda/n, i=arr[np.mod(t, n)])\n",
    "        sumGrad_sq = sumGrad_sq + grad**2\n",
    "        D = np.diag(1/np.sqrt(delta + sumGrad_sq.ravel()))\n",
    "        w = w - eta*D.dot(grad)   # gradient step\n",
    "            \n",
    "        # One epoch has passed: check for convergence\n",
    "        if np.mod(t, n) == 0:\n",
    "            change = np.linalg.norm(w-w_old, ord=np.inf)\n",
    "            print('Passes = %d, function = %e, change = %f' %((t+1)/n, self.LogReg(w=w, Lambda=Lambda)[0], change))\n",
    "            if change < self.progTol:\n",
    "                print('Parameters changed b less than progress tolerance on pass')\n",
    "                break\n",
    "            np.random.shuffle(arr)   # reshuffle\n",
    "            w_old = w\n",
    "            \n",
    "LogisticRegressionSGD.AdaGrad = AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes = 1, function = 2.724693e+04, change = 0.753612\n",
      "Passes = 2, function = 2.715153e+04, change = 0.081374\n",
      "Passes = 3, function = 2.712245e+04, change = 0.041782\n",
      "Passes = 4, function = 2.710405e+04, change = 0.028955\n",
      "Passes = 5, function = 2.709053e+04, change = 0.021070\n",
      "Passes = 6, function = 2.708448e+04, change = 0.018139\n",
      "Passes = 7, function = 2.708505e+04, change = 0.011846\n",
      "Passes = 8, function = 2.707827e+04, change = 0.010523\n",
      "Passes = 9, function = 2.707679e+04, change = 0.009694\n",
      "Passes = 10, function = 2.707562e+04, change = 0.006893\n"
     ]
    }
   ],
   "source": [
    "LogisticRegressionSGD(X, y).AdaGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Adaptive Gradient (SAG)\n",
    "\n",
    "Last but not least, we now discuss the [SAG algorithm](https://arxiv.org/pdf/1309.2388v2.pdf), a variant on batching in SGD that was published in 2015. The basic implementation of this method can be explained schematically as follows:\n",
    "\n",
    "1. Randomly select $i_t$ and compute the gradient $\\nabla f_{i_t} (w_t)$.\n",
    "\n",
    "2. Update the weights by taking a step towards the average of all the gradients computed so far\n",
    "\n",
    "    $$ w_{t+1} = w_t - \\eta_t \\left( \\frac{1}{n} \\sum_{i=1}^n G_i^t \\right), $$\n",
    "    \n",
    "    where $G_i^t$ keeps in memory all the gradients $\\nabla f_{i_t} (w)$ computed before iteration $t$ (with replacement if training example $i_t$ is visited repeatedly).\n",
    "    \n",
    "3. Repeat.\n",
    "\n",
    "Additionally, in contrast the the methods outlined before, SAG also leverages a property we have not used so far: Lipschitz continuity in the gradient of convex cost functions $f$\n",
    "\n",
    "$$ \\lVert \\nabla f(x) - \\nabla f(y) \\lVert \\; \\leq \\; L \\lVert x - y \\lVert. $$\n",
    "\n",
    "By choosing the learning rate to be inversely proportional to the maximal Lipschitz constant over all training examples\n",
    "\n",
    "$$ L = \\frac{1}{4} \\max_{1 \\leq i \\leq n} \\left( \\lVert x^i \\lVert^2 \\right) + \\lambda, \\;\\;\\; \\eta_t = 1/L, $$\n",
    "\n",
    "(here $x^i$ denotes a row of $X$), SAG achieves vastly superior convergence than all of the methods discussed above. In fact it is the only method of the ones outlined in this post that converges to the global minimum to 5 significant figures.\n",
    "\n",
    "#### A caveat on randomness\n",
    "\n",
    "The implementation of SAG below approaches the random updating of the gradients in two different ways, with surprising consequences.\n",
    "\n",
    "1. In `case = 1`, the index $i_t$ is sampled *randomly with replacement*, meaning that not all training examples are necessarily visited after an epoch has been completed. This choice of sampling leads to the best convergence properties. \n",
    "\n",
    "2. In `case = 2`, the index $i_t$ is sampled *randomly without replacement*, such that all training examples are cycled through exactly once during each pass. It turns out that simply reshuffling the cycle after each pass, the method of choice for all the methods above, actually yields a **much worse** performance for SAG. \n",
    "\n",
    "It can be verified that random sampling with replacement barely affects the other SGD algorithms, but it remains somewhat of a mystery to me why this choice in randomness affects convergence so drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SAG(self, case=1):\n",
    "    # Initialize\n",
    "    n = self.n\n",
    "    d = self.d\n",
    "    w = np.zeros((d, 1))\n",
    "    w_old = w\n",
    "    Lambda = 1\n",
    "    \n",
    "    # Randomly shuffle training data\n",
    "    arr = np.arange(0, n)\n",
    "    np.random.shuffle(arr) # shuffles arr directly\n",
    "    \n",
    "    # SAG parameters\n",
    "    G = np.zeros((n, d))\n",
    "    dvec = np.zeros((d, 1))\n",
    "    \n",
    "    L = 0.25*np.max(np.sum(self.X**2, axis=1)) + Lambda\n",
    "    eta = 1/L\n",
    "    \n",
    "    # strange property of random numbers with SAG\n",
    "    if case==1:\n",
    "        # much faster to generate all at once\n",
    "        arr = np.random.randint(n, size=(n,))\n",
    "    elif case==2:\n",
    "        arr = np.arange(0, n)\n",
    "        np.random.shuffle(arr) # shuffles arr directly\n",
    "    else:\n",
    "        print(\"The variable 'case' is not specified correctly; abort.\")\n",
    "        return\n",
    "    \n",
    "    for t in range(1, self.nEpochs*n + 1):\n",
    "        # Compute grad for one random training example\n",
    "        i = arr[np.mod(t, n)]\n",
    "        # i = np.random.randint(n)\n",
    "        grad = self.LogReg(w=w, Lambda=Lambda/n, i=i)[1]\n",
    "\n",
    "        # SAG algorithm\n",
    "        dvec = dvec - G[[i], :].T + grad\n",
    "        G[[i], :] = grad.T\n",
    "        w = w - eta*dvec/n\n",
    "            \n",
    "        # One epoch has passed: check for convergence\n",
    "        if np.mod(t, n) == 0:\n",
    "            change = np.linalg.norm(w-w_old, ord=np.inf)\n",
    "            print('Passes = %d, function = %e, change = %f' %((t+1)/n, self.LogReg(w=w, Lambda=Lambda)[0], change))\n",
    "            if change < self.progTol:\n",
    "                print('Parameters changed by less than progress tolerance on pass')\n",
    "                break\n",
    "            w_old = w\n",
    "            # careful with random numbers\n",
    "            if case==1:\n",
    "                arr = np.random.randint(n, size=(n,))\n",
    "            elif case==2:\n",
    "                np.random.shuffle(arr) # shuffles arr directly\n",
    "                \n",
    "LogisticRegressionSGD.SAG = SAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "  Case 1: completely random walk through training examples       \n",
      "-----------------------------------------------------------------\n",
      "Passes = 1, function = 2.827896e+04, change = 0.800740\n",
      "Passes = 2, function = 2.750555e+04, change = 0.333843\n",
      "Passes = 3, function = 2.719657e+04, change = 0.231760\n",
      "Passes = 4, function = 2.715866e+04, change = 0.142823\n",
      "Passes = 5, function = 2.710138e+04, change = 0.073561\n",
      "Passes = 6, function = 2.708099e+04, change = 0.047769\n",
      "Passes = 7, function = 2.707263e+04, change = 0.039457\n",
      "Passes = 8, function = 2.706958e+04, change = 0.015866\n",
      "Passes = 9, function = 2.706824e+04, change = 0.011306\n",
      "Passes = 10, function = 2.706798e+04, change = 0.006513\n",
      "-----------------------------------------------------------------\n",
      "  Case 2: visiting every training example exactly once per pass  \n",
      "-----------------------------------------------------------------\n",
      "Passes = 1, function = 2.997070e+04, change = 1.087841\n",
      "Passes = 2, function = 2.791141e+04, change = 0.457067\n",
      "Passes = 3, function = 2.798343e+04, change = 0.471225\n",
      "Passes = 4, function = 2.926349e+04, change = 0.359653\n",
      "Passes = 5, function = 2.934598e+04, change = 0.418543\n",
      "Passes = 6, function = 2.826330e+04, change = 0.320090\n",
      "Passes = 7, function = 2.722061e+04, change = 0.136731\n",
      "Passes = 8, function = 2.847064e+04, change = 0.192135\n",
      "Passes = 9, function = 3.097148e+04, change = 0.350279\n",
      "Passes = 10, function = 3.087167e+04, change = 0.466785\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------------------------------------------')\n",
    "print('  Case 1: completely random walk through training examples       ')\n",
    "print('-----------------------------------------------------------------')\n",
    "LogisticRegressionSGD(X, y).SAG(case=1)\n",
    "print('-----------------------------------------------------------------')\n",
    "print('  Case 2: visiting every training example exactly once per pass  ')\n",
    "print('-----------------------------------------------------------------')\n",
    "LogisticRegressionSGD(X, y).SAG(case=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas regular stochastic gradient descent has been known for a long time, some of the variants discussed in this post are quite recent: AdaGrad and SAG were first described in peer-reviewed publications in 2011 and 2015 respectively. It is interesting to note that the latter is already one of the main solvers available for Logistic and Ridge regressions in [scikit-learn](http://scikit-learn.org/stable/index.html) because of its efficiency with large datasets. The rapid rise of online learning and artificial neural networks, where stochastic optimization shines brightest, are sure to stimulate research for even better methods in the near future. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
